import os
import fitz  # PyMuPDF
import streamlit as st
import tempfile
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import google.generativeai as genai
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv()
genai.configure(api_key=os.getenv("AIzaSyCtN5qIj5G8R1bVS9dFRZvixj5Fy8h33XE"))
model = genai.GenerativeModel("gemini-1.5-flash")

# ì„ë² ë”© ëª¨ë¸ (CPU ì „ìš©)
embedder = SentenceTransformer("all-MiniLM-L6-v2", device="cpu")

# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_by_page(pdf_path):
    doc = fitz.open(pdf_path)
    return [page.get_text() for page in doc]

# ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
def create_faiss_index(texts):
    embeddings = embedder.encode(texts)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings).astype('float32'))
    return index, embeddings

# ìš”ì•½ í•¨ìˆ˜ (í•œêµ­ì–´/ì˜ì–´)
def summarize(text):
    korean_prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\n{text[:4000]}"
    english_prompt = f"Please summarize the following in English:\n\n{text[:4000]}"
    kr = model.generate_content(korean_prompt).text.strip()
    en = model.generate_content(english_prompt).text.strip()
    return kr, en

# ì§ˆë¬¸ ì‘ë‹µ í•¨ìˆ˜
def answer_question(text, question):
    prompt = f"""ë‹¤ìŒì€ ë¬¸ì„œì˜ ì¼ë¶€ì…ë‹ˆë‹¤:

{text[:4000]}

ì§ˆë¬¸: {question}
ë‹µë³€:"""
    return model.generate_content(prompt).text.strip()

# Streamlit UI
st.set_page_config(page_title="ğŸ“„ PDF êµ¬ê°„ ìš”ì•½ ë° ì§ˆë¬¸", layout="wide")
st.title("ğŸ“„ PDF êµ¬ê°„ ìš”ì•½ ë° ì§ˆë¬¸ ì±—ë´‡")

uploaded_file = st.file_uploader("ğŸ“¤ PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])

if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        pdf_path = tmp_file.name

    all_pages = extract_text_by_page(pdf_path)
    total_pages = len(all_pages)
    st.success(f"ì´ {total_pages} í˜ì´ì§€ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    start_page = st.number_input("ì‹œì‘ í˜ì´ì§€ (1ë¶€í„° ì‹œì‘)", min_value=1, max_value=total_pages, value=1)
    end_page = st.number_input("ë í˜ì´ì§€", min_value=1, max_value=total_pages, value=start_page)

    if start_page <= end_page:
        selected_texts = all_pages[start_page - 1:end_page]
        section_text = "\n".join(selected_texts)

        st.subheader("ğŸ“˜ êµ¬ê°„ ìš”ì•½")
        if st.button("ìš”ì•½ ì‹œì‘"):
            with st.spinner("ìš”ì•½ ì¤‘..."):
                kr_sum, en_sum = summarize(section_text)
                st.markdown("### ğŸ‡°ğŸ‡· í•œêµ­ì–´ ìš”ì•½")
                st.write(kr_sum)
                st.markdown("### ğŸ‡ºğŸ‡¸ ì˜ì–´ ìš”ì•½")
                st.write(en_sum)

        st.subheader("ğŸ’¬ êµ¬ê°„ ì§ˆë¬¸")
        question = st.text_input("ì§ˆë¬¸ ì…ë ¥:")
        if question:
            with st.spinner("ì§ˆë¬¸ ì‘ë‹µ ì¤‘..."):
                index, embeddings = create_faiss_index(selected_texts)
                query_embedding = embedder.encode([question])
                similarities = cosine_similarity(query_embedding, embeddings)[0]
                top_indices = similarities.argsort()[-3:][::-1]
                top_texts = [selected_texts[i] for i in top_indices]
                context = "\n\n".join(top_texts)
                answer = answer_question(context, question)
                st.markdown("### ğŸ¤– ë‹µë³€")
                st.write(answer)
    else:
        st.warning("âš ï¸ ì‹œì‘ í˜ì´ì§€ëŠ” ë í˜ì´ì§€ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.")
